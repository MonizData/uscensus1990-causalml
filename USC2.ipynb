{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23811aae-c5a6-4450-a068-2fc274f34c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22669d45-2926-4aa9-b2c0-8d4ee8902620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file (no header)\n",
    "df = pd.read_csv(\"USCensus1990.csv\", header=None)\n",
    "\n",
    "# Define column names (caseid + 68 attributes)\n",
    "attribute_names = [\n",
    "    \"dAge\", \"dAncstry1\", \"dAncstry2\", \"iAvail\", \"iCitizen\", \"iClass\", \"dDepart\", \"iDisabl1\", \"iDisabl2\", \"iEnglish\",\n",
    "    \"iFeb55\", \"iFertil\", \"dHispanic\", \"dHour89\", \"dHours\", \"iImmigr\", \"dIncome1\", \"dIncome2\", \"dIncome3\", \"dIncome4\",\n",
    "    \"dIncome5\", \"dIncome6\", \"dIncome7\", \"dIncome8\", \"dIndustry\", \"iKorean\", \"iLang1\", \"iLooking\", \"iMarital\",\n",
    "    \"iMay75880\", \"iMeans\", \"iMilitary\", \"iMobility\", \"iMobillim\", \"dOccup\", \"iOthrserv\", \"iPerscare\", \"dPOB\",\n",
    "    \"dPoverty\", \"dPwgt1\", \"iRagechld\", \"dRearning\", \"iRelat1\", \"iRelat2\", \"iRemplpar\", \"iRiders\", \"iRlabor\",\n",
    "    \"iRownchld\", \"dRpincome\", \"iRPOB\", \"iRrelchld\", \"iRspouse\", \"iRvetserv\", \"iSchool\", \"iSept80\", \"iSex\", \"iSubfam1\",\n",
    "    \"iSubfam2\", \"iTmpabsnt\", \"dTravtime\", \"iVietnam\", \"dWeek89\", \"iWork89\", \"iWorklwk\", \"iWWII\", \"iYearsch\",\n",
    "    \"iYearwrk\", \"dYrsserv\"\n",
    "]\n",
    "df.columns = ['caseid'] + attribute_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5c61f-fdec-48bb-bc88-196690db49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extreme outlier \n",
    "df = df[df['dHours'] != 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d000a3-6ac7-4e21-94a1-ed2127dc0b37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove 'caseid' \n",
    "df = df.drop(columns=['caseid'])\n",
    "\n",
    "# Remove highly correlated columns\n",
    "X = df.drop(columns=['dHours', 'dIncome1', 'dHour89', 'dRearning'], errors='ignore')\n",
    "y = df['dHours']\n",
    "\n",
    "# Verify non-numerical columns\n",
    "non_numeric = X.select_dtypes(exclude=['number']).columns\n",
    "print(\"Non-numerical columns:\", non_numeric.tolist())\n",
    "\n",
    "# Remove non-numerical columns\n",
    "X = X.drop(columns=non_numeric)\n",
    "\n",
    "# Fill NAs\n",
    "X = X.fillna(X.median(numeric_only=True))\n",
    "\n",
    "# Scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da27c4-dc23-4218-97fa-7883a25b5985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(64, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Regression output\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a563c7-70cd-4152-8299-167500d8a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict 10 random examples from the test set\n",
    "\n",
    "predictions = model.predict(X_test[:10])\n",
    "for i in range(10):\n",
    "    print(f\"Predicted: {predictions[i][0]:.1f} | Actual: {y_test.iloc[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbdf40b-ad7c-4f4d-a464-496a66a4454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample (reduces memory and CPU use)\n",
    "X_sub = X.iloc[:2000, : ]\n",
    "y_sub = y.iloc[:2000]\n",
    "\n",
    "# Compute MI\n",
    "mi_scores = mutual_info_regression(X_sub, y_sub, discrete_features='auto')\n",
    "mi_series = pd.Series(mi_scores, index=X_sub.columns).sort_values(ascending=False)\n",
    "\n",
    "# Show top 5\n",
    "top_features = mi_series.head(5)\n",
    "for i, (feature, score) in enumerate(top_features.items(), 1):\n",
    "    print(f\"{i}. {feature} — MI Score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0097e930-9fd8-4273-87f2-bc2c2e1b78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample still (e.g., 2000 rows), but use all columns\n",
    "X_sub = X.iloc[:2000, :]\n",
    "y_sub = y.iloc[:2000]\n",
    "\n",
    "# Compute Mutual Information\n",
    "mi_scores = mutual_info_regression(X_sub, y_sub, discrete_features='auto')\n",
    "mi_series = pd.Series(mi_scores, index=X_sub.columns).sort_values(ascending=False)\n",
    "\n",
    "# Show top 10\n",
    "top_features = mi_series.head(10)\n",
    "top_features.plot(kind='bar', title='Top 10 Features by Mutual Information', ylabel='MI Score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e44be3-11d1-4ca9-a652-2ae668edb323",
   "metadata": {},
   "source": [
    "Reduce to top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248625eb-dd36-4add-ab31-686ea752abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the top N features from MI\n",
    "top_N = 10\n",
    "selected_features = mi_series.head(top_N).index.tolist()\n",
    "\n",
    "# Redefine X with only those features\n",
    "X_reduced = X[selected_features]\n",
    "\n",
    "# Normalization again (Min-Max)\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled_reduced = scaler.fit_transform(X_reduced)\n",
    "\n",
    "# Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_reduced, y, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8b00b-7317-4ee8-83ae-d508260272d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5dd2fd-4823-4d8c-a4b8-1f333c42d3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create treatment variable (binary)\n",
    "df['treated'] = (df['dHours'] <= 2).astype(int)\n",
    "\n",
    "# Choose covariates (ex: top 10 MI features)\n",
    "covariates = selected_features  # from MI ranking\n",
    "X_covariates = df[covariates]\n",
    "X_covariates = X_covariates.fillna(X_covariates.median(numeric_only=True))\n",
    "\n",
    "# Target outcome to evaluate (e.g., income or other)\n",
    "outcome = 'dIncome1'\n",
    "y_outcome = df[outcome]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb87eb5-95d7-41db-8bba-71f56714ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['dHours'].describe())\n",
    "print(df['treated'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf7fef-8650-49b2-aead-4c97057a1cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit logistic regression to estimate propensity scores\n",
    "ps_model = LogisticRegression(max_iter=1000)\n",
    "ps_model.fit(X_covariates, df['treated'])\n",
    "\n",
    "# Get propensity scores\n",
    "df['propensity_score'] = ps_model.predict_proba(X_covariates)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90ab1e-8adc-411a-a9ed-7954621ee9b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split treated and control\n",
    "treated = df[df['treated'] == 1]\n",
    "control = df[df['treated'] == 0]\n",
    "\n",
    "# Fit nearest neighbors on control group\n",
    "nn = NearestNeighbors(n_neighbors=1)\n",
    "nn.fit(control[['propensity_score']])\n",
    "\n",
    "# Find nearest neighbor for each treated unit\n",
    "distances, indices = nn.kneighbors(treated[['propensity_score']])\n",
    "matched_control = control.iloc[indices.flatten()]\n",
    "\n",
    "# Compute ATE\n",
    "ate = (treated[outcome].reset_index(drop=True) - matched_control[outcome].reset_index(drop=True)).mean()\n",
    "print(f\"Estimated ATE (dHours <= 2 on {outcome}): {ate:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07369ee-0639-47e8-a4bc-dcf1ca6047d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define covariates, treatment, and outcome\n",
    "X = df[selected_features].copy()\n",
    "y = df[\"treated\"]\n",
    "outcome = df[\"dIncome1\"]\n",
    "\n",
    "# 2. Handle missing values and scale features\n",
    "X = X.fillna(X.median(numeric_only=True))\n",
    "X_scaled = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# 3. Train Random Forest classifier to estimate propensity scores\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=123)\n",
    "rf.fit(X_scaled, y)\n",
    "propensity_scores = rf.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "# 4. Add propensity scores to the DataFrame\n",
    "df_psm = df.copy()\n",
    "df_psm[\"propensity_score\"] = propensity_scores\n",
    "df_psm[\"dIncome1\"] = outcome\n",
    "df_psm[\"treated\"] = y.values\n",
    "\n",
    "# 5. Separate treated and control groups\n",
    "treated_df = df_psm[df_psm[\"treated\"] == 1].copy()\n",
    "control_df = df_psm[df_psm[\"treated\"] == 0].copy()\n",
    "\n",
    "# 6. Match each treated unit to the nearest control based on propensity score\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm=\"ball_tree\").fit(control_df[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated_df[[\"propensity_score\"]])\n",
    "matched_controls = control_df.iloc[indices.flatten()].copy()\n",
    "\n",
    "# 7. Estimate the Average Treatment Effect (ATE)\n",
    "treated_outcomes = treated_df[\"dIncome1\"].values\n",
    "control_outcomes = matched_controls[\"dIncome1\"].values\n",
    "ate = np.mean(treated_outcomes - control_outcomes)\n",
    "\n",
    "print(f\"Estimated ATE (treated vs. matched control on dIncome1): {ate:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a4552e-15d1-4dca-923a-cb3940e7a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate CATE (Conditional Average Treatment Effect) by subgroup using correct column names\n",
    "\n",
    "# Create age groups based on dAge\n",
    "df['age_group'] = pd.cut(df['dAge'], bins=[0, 25, 40, 60, 100], labels=['<=25', '26-40', '41-60', '60+'])\n",
    "\n",
    "# Define subgroup columns based on your dataset\n",
    "subgroups = {\n",
    "    'Sex': 'iSex',\n",
    "    'Education': 'iSchool',\n",
    "    'Age Group': 'age_group'\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, col in subgroups.items():\n",
    "    for level in df[col].dropna().unique():\n",
    "        df_sub = df[df[col] == level].copy()\n",
    "\n",
    "        if df_sub['treated'].nunique() < 2:\n",
    "            continue\n",
    "\n",
    "        covariates = df_sub.drop(columns=['treated', 'dIncome1', 'dHours', 'dHour89', 'dRearning', 'dWeek89', 'propensity_score'], errors='ignore')\n",
    "        covariates = covariates.select_dtypes(include=[np.number]).fillna(0)\n",
    "\n",
    "        if covariates.shape[1] == 0:\n",
    "            continue\n",
    "\n",
    "        ps_model = LogisticRegression(max_iter=1000)\n",
    "        ps_model.fit(covariates, df_sub['treated'])\n",
    "        df_sub['propensity_score'] = ps_model.predict_proba(covariates)[:, 1]\n",
    "\n",
    "        treated = df_sub[df_sub['treated'] == 1]\n",
    "        control = df_sub[df_sub['treated'] == 0]\n",
    "\n",
    "        if len(treated) == 0 or len(control) == 0:\n",
    "            continue\n",
    "\n",
    "        nn = NearestNeighbors(n_neighbors=1)\n",
    "        nn.fit(control[['propensity_score']])\n",
    "        _, indices = nn.kneighbors(treated[['propensity_score']])\n",
    "        matched_controls = control.iloc[indices.flatten()]\n",
    "\n",
    "        ate = (treated['dIncome1'].values - matched_controls['dIncome1'].values).mean()\n",
    "\n",
    "        results.append({\n",
    "            'Subgroup': name,\n",
    "            'Level': level,\n",
    "            'CATE': ate,\n",
    "            'N Treated': len(treated),\n",
    "            'N Control': len(control)\n",
    "        })\n",
    "\n",
    "cate_df = pd.DataFrame(results)\n",
    "display(cate_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5923f-7b04-4ea5-87ba-f126a0ee0538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Map readable names to actual DataFrame column names\n",
    "subgroup_map = {\n",
    "    'Sex': 'iSex',\n",
    "    'Education': 'iSchool',\n",
    "    'Age Group': 'age_group'\n",
    "}\n",
    "\n",
    "# Ensure 'age_group' is a string if it exists\n",
    "if 'age_group' in df.columns:\n",
    "    df['age_group'] = df['age_group'].astype(str)\n",
    "\n",
    "# Define CATE results (pre-calculated)\n",
    "cate_df = pd.DataFrame({\n",
    "    'Subgroup': ['Sex', 'Sex', 'Education', 'Education', 'Education', 'Age Group'],\n",
    "    'Level': [1, 0, 1, 2, 3, '<=25'],\n",
    "    'CATE': [-0.392255, -0.141909, 0.942493, -0.691051, 0.459430, -0.270070],\n",
    "    'N Treated': [11801, 9337, 13981, 5140, 912, 20802],\n",
    "    'N Control': [997, 2448, 3204, 179, 62, 3446]\n",
    "})\n",
    "\n",
    "# Perform Welch’s t-test for each subgroup\n",
    "p_values = []\n",
    "for _, row in cate_df.iterrows():\n",
    "    subgroup = row['Subgroup']\n",
    "    level = row['Level']\n",
    "    col = subgroup_map[subgroup]\n",
    "    level = str(level) if col == 'age_group' else int(level)\n",
    "\n",
    "    treated_vals = df[(df['treated'] == 1) & (df[col] == level)]['dIncome1']\n",
    "    control_vals = df[(df['treated'] == 0) & (df[col] == level)]['dIncome1']\n",
    "\n",
    "    t_stat, p_val = ttest_ind(treated_vals, control_vals, equal_var=False, nan_policy='omit')\n",
    "    p_values.append(p_val)\n",
    "\n",
    "# Add results to CATE DataFrame\n",
    "cate_df['p-value'] = p_values\n",
    "cate_df['Significant (p < 0.05)'] = cate_df['p-value'] < 0.05\n",
    "\n",
    "# show table\n",
    "display(cate_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30676446-f80a-41c1-9850-9712379cee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "s\n",
    "X = df[selected_features].copy()\n",
    "y = df[\"dIncome1\"]\n",
    "\n",
    "# Preencher valores em falta\n",
    "X = X.fillna(X.median(numeric_only=True))\n",
    "\n",
    "# scale\n",
    "X_scaled = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Train model\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor.fit(X_scaled, y)\n",
    "\n",
    "# Importância das features\n",
    "importances = rf_regressor.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# dataframe of importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')\n",
    "plt.title('Feature Importance from Random Forest Regressor')\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90cfe8c-16f3-4764-828c-0c4d35b01b5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare groups\n",
    "df['age_group'] = pd.cut(df['dAge'], bins=[0, 25, 40, 60, 100], labels=['<=25', '26-40', '41-60', '60+'])\n",
    "subgroups = {'Sex': 'iSex', 'Education': 'iSchool', 'Age Group': 'age_group'}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, col in subgroups.items():\n",
    "    for level in df[col].dropna().unique():\n",
    "        df_sub = df[df[col] == level].copy()\n",
    "        if df_sub['treated'].nunique() < 2:\n",
    "            continue\n",
    "\n",
    "        # Covariates para modelo de propensão\n",
    "        covariates = df_sub.drop(columns=['treated', 'dIncome1', 'dHours', 'dHour89', 'dRearning', 'dWeek89'], errors='ignore')\n",
    "        covariates = covariates.select_dtypes(include=[np.number]).fillna(0)\n",
    "\n",
    "        if covariates.shape[1] == 0:\n",
    "            continue\n",
    "\n",
    "        # estimate propensity score\n",
    "        ps_model = LogisticRegression(max_iter=5000)\n",
    "        ps_model.fit(covariates, df_sub['treated'])\n",
    "        df_sub['propensity_score'] = ps_model.predict_proba(covariates)[:, 1]\n",
    "\n",
    "        treated = df_sub[df_sub['treated'] == 1]\n",
    "        control = df_sub[df_sub['treated'] == 0]\n",
    "\n",
    "        if len(treated) == 0 or len(control) == 0:\n",
    "            continue\n",
    "\n",
    "        nn = NearestNeighbors(n_neighbors=1)\n",
    "        nn.fit(control[['propensity_score']])\n",
    "        _, indices = nn.kneighbors(treated[['propensity_score']])\n",
    "        matched_controls = control.iloc[indices.flatten()].copy()\n",
    "\n",
    "        cate = (treated['dIncome1'].values - matched_controls['dIncome1'].values).mean()\n",
    "\n",
    "        # Welch t-test \n",
    "        t_vals = df[(df['treated'] == 1) & (df[col] == level)]['dIncome1']\n",
    "        c_vals = df[(df['treated'] == 0) & (df[col] == level)]['dIncome1']\n",
    "        t_stat, p_val = ttest_ind(t_vals, c_vals, equal_var=False, nan_policy='omit')\n",
    "\n",
    "        results.append({\n",
    "            'Subgroup': name,\n",
    "            'Level': level,\n",
    "            'CATE': cate,\n",
    "            'N Treated': len(treated),\n",
    "            'N Control': len(control),\n",
    "            'p-value': p_val,\n",
    "            'Significant (p < 0.05)': p_val < 0.05\n",
    "        })\n",
    "\n",
    "# create dataframe\n",
    "cate_df = pd.DataFrame(results)\n",
    "cate_df = cate_df.sort_values(by=['Subgroup', 'Level']).reset_index(drop=True)\n",
    "\n",
    "# show on notebook\n",
    "styled_table = cate_df.style.format({\n",
    "    'CATE': '{:.3f}',\n",
    "    'p-value': '{:.4e}'\n",
    "}).background_gradient(subset=['CATE'], cmap='coolwarm').highlight_null()\n",
    "\n",
    "display(styled_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9681bc7-f106-47eb-8f8f-220df1bdcb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "features_to_check = selected_features\n",
    "\n",
    "# Before do matching\n",
    "df['group'] = df['treated']\n",
    "scaler = StandardScaler()\n",
    "smds_before = []\n",
    "\n",
    "for feature in features_to_check:\n",
    "    treated_vals = df[df['group'] == 1][feature].dropna()\n",
    "    control_vals = df[df['group'] == 0][feature].dropna()\n",
    "\n",
    "    smd = abs(treated_vals.mean() - control_vals.mean()) / np.sqrt((treated_vals.std()**2 + control_vals.std()**2) / 2)\n",
    "    smds_before.append(smd)\n",
    "\n",
    "# After matching\n",
    "\n",
    "\n",
    "treated = df[df['treated'] == 1].copy()\n",
    "control = df[df['treated'] == 0].copy()\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1)\n",
    "nn.fit(control[['propensity_score']])\n",
    "_, indices = nn.kneighbors(treated[['propensity_score']])\n",
    "matched_control = control.iloc[indices.flatten()].copy()\n",
    "\n",
    "smds_after = []\n",
    "\n",
    "for feature in features_to_check:\n",
    "    treated_vals = treated[feature].dropna()\n",
    "    control_vals = matched_control[feature].dropna()\n",
    "\n",
    "    smd = abs(treated_vals.mean() - control_vals.mean()) / np.sqrt((treated_vals.std()**2 + control_vals.std()**2) / 2)\n",
    "    smds_after.append(smd)\n",
    "\n",
    "# Create table\n",
    "smd_df = pd.DataFrame({\n",
    "    'Feature': features_to_check,\n",
    "    'SMD Before Matching': smds_before,\n",
    "    'SMD After Matching': smds_after\n",
    "})\n",
    "\n",
    "# Show table\n",
    "print(\"\\nTable – Standardized Mean Differences Before and After Matching\")\n",
    "display(smd_df.round(3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958e9e4-bf80-4636-9a09-a3a3c91c8076",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Before matching\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(df[df['treated'] == 1]['propensity_score'], label='Treated', fill=True, alpha=0.5)\n",
    "sns.kdeplot(df[df['treated'] == 0]['propensity_score'], label='Control', fill=True, alpha=0.5)\n",
    "plt.title(\"Before Matching\")\n",
    "plt.xlabel(\"Propensity Score\")\n",
    "plt.legend()\n",
    "\n",
    "# After matching\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(treated['propensity_score'], label='Treated', fill=True, alpha=0.5)\n",
    "sns.kdeplot(matched_control['propensity_score'], label='Matched Control', fill=True, alpha=0.5)\n",
    "plt.title(\"After Matching\")\n",
    "plt.xlabel(\"Propensity Score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Figure 3.2 – Propensity Score Distribution Before and After Matching\", y=1.05)\n",
    "plt.savefig(\"Figure_3_2_Propensity_Matching.png\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
